{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE 120 Lab 8: Shazam\n",
    "v1 - Spring 2020: Anmol Parande, Dominic Carrano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "In 2002, Shazam Entertainment Limited (founded by UC Berkeley students!) launched its music identification product, allowing users to dial a phone number and play a song. Then, they'd get a text message with the name of the song and its artist. In 2018, Shazam was acquired by Apple for \\$400 million, and it's now in every iPhone.\n",
    "\n",
    "Shazam works by using *audio fingerprinting*: given a song, it generates a set of identifiers, and searches an audio database to find a match and identify the song. In this lab, you'll learn about audio fingerprinting, and use it to build a music identification just like Shazam!\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "To get started, you'll need to install [Pandas](https://pandas.pydata.org/docs/getting_started/install.html). And, of course, you'll also need NumPy, SciPy, and Matplotlib if you don't already have them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as signal\n",
    "import pandas as pd\n",
    "import IPython.display as ipd\n",
    "\n",
    "from scipy.io import wavfile\n",
    "from scipy.ndimage import maximum_filter\n",
    "from shazam_utils import generate_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: To avoid any copyright issues, we've cropped all provided songs to only contain the first 60 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Spectral Analysis\n",
    "\n",
    "As we've already seen throughout the course, a signal's constituent frequencies tell us a lot about it. The same is true of audio: to find the salient features of songs to fingerprint, we'll need to look at the song's spectrum (i.e., Fourier Transform). Fortunately, we have the DFT (efficiently implemented via the FFT) to help us do this.\n",
    "\n",
    "To get started, let's load in *Viva La Vida* by Coldplay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs, coldplay = wavfile.read(\"VivaLaVida.wav\")\n",
    "print(\"Audio Shape: {0}, Sampling Rate: {1} Hz\".format(coldplay.shape, fs))\n",
    "coldplay = np.mean(coldplay, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, we think of audio as a two-channel, continuous signal $\\vec{x}(t) = \\left[x_L(t) \\ x_R(t)\\right]$, with one column of the *audio matrix* per channel. That is, $x_L(t)$ is the left channel's signal, and $x_R(t)$ the right channel's signal. The reason we have two distinct audio channels is so that we can have two streams playing at the same time, one per ear (e.g., in a pair of headphones or laptop speakers).\n",
    "\n",
    "We sample this CT audio signal at a particular rate (here, 48000 Hz) to get a DT signal. For our purposes, the distinction between our channels is not very important, so we'll just average them to form a 1D signal, $x(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense for the song we're working with, feel free to have a listen! This cell may take a few seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(\"VivaLaVida.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1a: One DFT is Not Enough\n",
    "\n",
    "As far as spectral analysis is concerned, it seems like we should just be able to take the DFT of the entire song, find our fingerprints, and be done, right? Is that really all there is to Shazam? No, not quite. It may not be obvious, but there's a big issue with this approach that we'll explore now. So that our code doesn't take forever to run, we'll only look at the first 10 seconds of the song, but the issues we'll find here apply generally to the entire signal.\n",
    "\n",
    "To see why one DFT won't suffice, in the cell below:\n",
    "1. Define a new signal `coldplay_cropped` using the first 10 seconds of the signal `coldplay`. **Make sure to use the variable `fs` defined for you.**\n",
    "2. Compute `coldplay_freqs`, the magnitude of its spectrum; you may use numpy for this (check out [np.fft.fft](https://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.fft.html) and [np.abs](https://docs.scipy.org/doc/numpy/reference/generated/numpy.absolute.html)).\n",
    "3. Center the magnitude with [np.fft.fftshift](https://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.fftshift.html). By default, when you compute the FFT, the samples of the DTFT that are returned go from $0$ to $2\\pi$; fftshift moves them around so that they go from $-\\pi$ to $\\pi$, which is nicer for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the magnitude spectrum of the first 10 seconds of Viva La Vida\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the spectrum of the first 10 seconds of the song looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4), dpi=200)\n",
    "freqs = np.linspace(-fs/2, fs/2, len(coldplay_freqs))\n",
    "plt.plot(freqs, coldplay_freqs)\n",
    "plt.xlabel(\"Frequency [Hz]\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.title(\"DFT of first 10 seconds of Viva La Vida\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the frequency content is centered around the lower frequencies. In fact, we can barely see anything past 10 kHz, because human hearing stops around 15-20 kHz (and generally decreases with age), so there's no reason to include anything that high in music."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, everything looks ok: we got the spectrum of the first 10 seconds of our song. This gives us a sort of \"aggregate view\" of the frequencies that show up at some point during the first 10 seconds. But is this \"aggregate view\" good enough? What happens if our signal is *non-stationary*, that is, is frequency content changes with time, as is certainly the case with music? \n",
    "\n",
    "To find out, in the cell below, define `coldplay_freqs_1`, `coldplay_freqs_2`, `coldplay_freqs_3`, `coldplay_freqs_4` as the centered (via fftshift) magnitude spectra of the first, second, third, and fourth seconds of the song, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use these variables to zoom in (temporally speaking) and inspect the song's frequency content over the course of a second of data (rather than 10), and see if the \"aggregate view\" gives a good enough picture of what frequencies are present at a specific second in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = np.linspace(-fs / 2, fs / 2, len(coldplay_freqs_1))\n",
    "sigs = [coldplay_freqs_1, coldplay_freqs_2, coldplay_freqs_3, coldplay_freqs_4]\n",
    "strs = [\"1st\", \"2nd\", \"3rd\", \"4th\"]\n",
    "\n",
    "plt.figure(figsize=(16, 10), dpi=200)\n",
    "for i in range(1, 5):\n",
    "    plt.subplot(2, 2, i)\n",
    "    plt.plot(freqs, sigs[i-1])\n",
    "    plt.xlim([-.5e4, .5e4])\n",
    "    plt.ylim([0, 1.1 * np.array(sigs).max()])\n",
    "    plt.title(\"DFT magnitude of {} second of Viva La Vida\".format(strs[i-1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how while most of the energy in each second's spectrum is concentrated inside $[-2.5 \\text{ kHz}, +2.5 \\text{ kHz}]$, the exact shapes are quite different. \n",
    "\n",
    "**The issue is that the aggregate view from our 10-second DFT doesn't have good enough *temporal resolution*: we can't see how the signal's frequency content changes over time!**\n",
    "\n",
    "Why does this matter, you ask? Well, when we're working with the real deal, we don't feed Shazam the entire song; only a clip. For example, suppose you tune into a radio station halfway through a song. Then, 20 seconds later, you think to yourself, \"hey, I like this\" and pull out Shazam to figure out what song it is. By then, whatever you're giving Shazam is missing a lot of data, and so it needs to be able to look at what frequencies are in the song at different points in time to correctly identify it. The aggregate view won't do. Fortunately, there's a very simple fix to this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1b: Spectrogrammin'\n",
    "\n",
    "The results of Q1a are pretty clear: we need a way to see how the signal's frequency content changes over time. Just taking one DFT of the entire signal fails to achieve this. Instead, we'll use a *spectrogram*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectrograms\n",
    "\n",
    "A *spectrogram* is an image representing the frequency content of a signal at different times. This ability to see how a signal's frequency content changes with time is the key useful feature of a spectrogram. \n",
    "\n",
    "To compute a spectrogram, we split our signal into chunks, compute the DFT of each chunk, and plot the magnitude squared of those DFT chunks side-by-side. To make visualization easier, we typically employ a colormap to distinguish where the DFT's squared-magnitude is bigger.\n",
    "\n",
    "For example, here is a spectrogram of speech, taken from [here](https://www.researchgate.net/figure/Spectrogram-of-a-speech-signal-with-breath-sound-marked-as-Breath-whose-bounds-are_fig1_319081627). The red areas correspond to stronger frequency content, and green areas to weaker frequency content.\n",
    "\n",
    "Notice the differences between when the speaker takes a breath and when the speaker is actually speaking. A single DFT wouldn't be able to separate this!\n",
    "\n",
    "![image.png](https://www.researchgate.net/profile/Sri_Harsha_Dumpala/publication/319081627/figure/fig1/AS:534034566004736@1504335170521/Spectrogram-of-a-speech-signal-with-breath-sound-marked-as-Breath-whose-bounds-are.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Job\n",
    "\n",
    "To get some familiarity with spectrograms, let's generate some sinusoidal signals and plot their spectrograms.\n",
    "\n",
    "In the cell below, generate 1000 samples of the following signals over the interval $t \\in [0, 1]$:\n",
    "- A 100 Hz sine wave (call it `x1`).\n",
    "- A 400 Hz sine wave (call it `x2`).\n",
    "\n",
    "Finally, create a third signal, call it `x3`, by concatenating `x1` and `x2`. \n",
    "\n",
    "As a reminder, a sine wave at frequency $f_0$ Hz is given by the equation $x(t) = \\sin(2\\pi f_0 t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate 3 sinusoidal signals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at the DFT of our signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_1 = np.abs(np.fft.fftshift(np.fft.fft(x1)))\n",
    "freqs_2 = np.abs(np.fft.fftshift(np.fft.fft(x2)))\n",
    "freqs_3 = np.abs(np.fft.fftshift(np.fft.fft(x3)))\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(16, 4), dpi=200)\n",
    "\n",
    "axs[0].plot(np.linspace(-500,500, len(freqs_1)), freqs_1)\n",
    "axs[0].set_ylabel('DFT Magnitude')\n",
    "axs[0].set_xlabel('Frequency [Hz]')\n",
    "axs[0].set_title(\"100 Hz Signal\")\n",
    "\n",
    "axs[1].plot(np.linspace(-500,500, len(freqs_2)), freqs_2)\n",
    "axs[1].set_ylabel('DFT Magnitude')\n",
    "axs[1].set_xlabel('Frequency [Hz]')\n",
    "axs[1].set_title(\"400 Hz Signal\")\n",
    "\n",
    "axs[2].plot(np.linspace(-500,500, len(freqs_3)), freqs_3)\n",
    "axs[2].set_ylabel('DFT Magnitude')\n",
    "axs[2].set_xlabel('Frequency [Hz]')\n",
    "axs[2].set_title(\"100 Hz and 400 Hz Signal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the *pure tones* (the 100 Hz and 400 Hz sine waves) have 2 peaks each, due to conjugate symmetry, whereas the signal formed by concatenating them has 4 peaks. Now, let's look at the spectrograms of these signals. Run the following code to plot the spectrogram of each signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1, t1, x1_freqs = signal.spectrogram(x1, fs=1000)\n",
    "f2, t2, x2_freqs = signal.spectrogram(x2, fs=1000)\n",
    "f3, t3, x3_freqs = signal.spectrogram(x3, fs=1000)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(16, 4), dpi=200)\n",
    "\n",
    "axs[0].pcolormesh(t1, f1, x1_freqs, cmap=\"gray\")\n",
    "axs[0].set_ylabel('Frequency [Hz]')\n",
    "axs[0].set_xlabel('Time [sec]')\n",
    "axs[0].set_title(\"100 Hz Signal\")\n",
    "\n",
    "axs[1].pcolormesh(t2, f2, x2_freqs, cmap=\"gray\")\n",
    "axs[1].set_ylabel('Frequency [Hz]')\n",
    "axs[1].set_xlabel('Time [sec]')\n",
    "axs[1].set_title(\"400 Hz Signal\")\n",
    "\n",
    "axs[2].pcolormesh(t3, f3, x3_freqs, cmap=\"gray\")\n",
    "axs[2].set_ylabel('Frequency [Hz]')\n",
    "axs[2].set_xlabel('Time [sec]')\n",
    "axs[2].set_title(\"100 Hz and 400 Hz Signal\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did this correctly, you should see 3 spectrograms. The first one will have a single band at 100 Hz. The second will have a single band at 400 Hz. The final one will have two bands (one at 100 Hz and one at 400 Hz). The reason we aren't seeing conjugate symmetry here is because we are only plotting the positive frequencies. For the most part, these spectrograms appear to give us the same information as the DFT. \n",
    "\n",
    "However, notice that in the 3rd spectrogram, the frequencies are mostly only present for the duration they exist. There's some overlap between 1.0-1.2 seconds, which isn't what we would have expected. This happens because SciPy doesn't truly use distinct chunks, as we mentioned above, and instead goes with a more sophisticated overlapping window approach, covered in EE 123 (this gives a better tradeoff between the temporal and spectral resolutions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1c: Spectrograms of Songs\n",
    "\n",
    "Now that we've got the basic concepts down, let's load *Viva La Vida* and *Mr. Brightside* and compare their spectrograms. Run the cell below to load the songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs, coldplay = wavfile.read(\"VivaLaVida.wav\")\n",
    "coldplay = np.mean(coldplay, axis=1)\n",
    "\n",
    "fs, killers = wavfile.read(\"MrBrightside.wav\")\n",
    "killers = np.mean(killers, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we haven't heard *Mr. Brightside* yet, let's load it in now and have a listen. This cell will take a few seconds to load before the audio interface shows up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(\"MrBrightside.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better looking image when visualizing the spectrogram, we'll plot everything in decibels. \n",
    "\n",
    "Recall that to convert a number $x$ to decibels, we compute $x_\\text{dB} = 20\\log_{10}(x).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Job\n",
    "\n",
    "In the cell below:\n",
    "1. Use [`signal.spectrogram`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.spectrogram.html) to compute the spectrogram of each song. \n",
    "    - Use 4096 for the `nperseg` parameter of `signal.spectrogram` to take a 4096 point DFT. This matches the length of DFT typically used in practical audio fingerprinting systems, representing a good tradeoff between spectral and temporal resolution.\n",
    "    - The function returns a tuple containing the frequencies of the spectrogram samples, time points of the spectrogram samples, and the actual spectrogram. For each call to `signal.spectrogram`, store these as `fi, ti, []_spect` where `i` is 1 or 2 and `[]` gets filled with `coldplay` or `killers` depending on the song.\n",
    "2. Convert the resultant spectrograms to the decibel scale using the formula from above.\n",
    "\n",
    "Don't worry about any divide-by-zero issues that happen due ot taking the log. If you're concerned, you can add a small positive constant (say, $10^{-12}$) as a remedy to silence the warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the spectrogram of Viva La Vida and Mr. Brightside in decibels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look! Run the cell below to plot the spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10), dpi=200)\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.pcolormesh(t1, f1, coldplay_spect, cmap=\"jet\")\n",
    "plt.ylabel(\"Frequency [Hz]\")\n",
    "plt.xlabel(\"Time [sec]\")\n",
    "plt.title(\"Viva La Vida\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.pcolormesh(t2, f2, killers_spect, cmap=\"jet\")\n",
    "plt.ylabel(\"Frequency [Hz]\")\n",
    "plt.xlabel(\"Time [sec]\")\n",
    "plt.title(\"Mr. Brightside\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** In both spectrograms, we see a column of dark blue for the first second or so. Based on our colorbar, it looks like this corresponds to $\\approx -300 \\text{dB}$, or essentially no signal power. In terms of the songs, why do we have this in our plots?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**A:** (TODO)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** At the beginning of the spectrogram for Mr. Brightside (after the column of dark blue), you should see two peaks that extend up toward $20 \\text{ kHz}$. What sound in the song is this part of the spectrogram capturing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**A:** (TODO)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Can you easily tell the two songs' spectrograms apart? Do you think they'd make good building blocks for our audio recognition algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**A:** (TODO)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Fingerprinting\n",
    "\n",
    "Our end goal here is to take an audio snippet, and figure out what song's being played. To do this, we'll need a database of songs to compare against. \n",
    "\n",
    "Should we just store entire songs in the database? Probably not, as that'd be a very large database: a three-minute WAV file sampled at $48 \\text{ kHz}$ is a about $30 \\text{ MB}$ in size. Even if we aimed for the modest goal of 1000 songs (which the original iPod from 2001 could hold), we're already looking at using over $30 \\text{ GB}$ of storage. Additionally, comparing raw audio samples for similarity isn't very robust against noise.\n",
    "\n",
    "Instead, we'll generate a set of *fingerprints* from each song, and store these in our database. When our version of Shazam gets fed a song to classify, it can just compare the fingerprints, rather than looking at the whole song. This should solve our storage issues, provided the fingerprints aren't too large. But clearly we'll need this fingerprinting algorithm to have a few other properties for this audio recognition system to be useful.\n",
    "\n",
    "In particular, we want our audio fingerprint to have four key properties:\n",
    "1. ***Temporal Locality:*** We're trying to figure out what song is being played based on a short (say, 5 to 10 second long) clip. So, our fingerprints should somehow encode *where* in the song they come from.\n",
    "\n",
    "2. ***Translational Invariance:*** The snippet we play for Shazam could come from anywhere in the song. We could play it the first 5 seconds, the last 5, or something in the middle. In all cases, we want a correct result, so the same chunk of audio should get the same fingerpint regardless of whether it shows up a minute into a clip or right at the beginning—it's the actual music in it that we should use to generate the fingerprint.\n",
    "\n",
    "3. ***Robustness:*** An audio file, whether clean or degraded by (a modest amount of) noise, should produce the same fingerprint.\n",
    "\n",
    "4. ***High Entropy:*** The fingerprinting algorithm should be \"random enough\" that two different songs don't produce the same fingerprint.\n",
    "\n",
    "As it turns out, spectrograms have all these nice properties, which is why they're such an important part of Shazam! The company's founders recognized this too, and discussed it in their original paper, linked in the references."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So, spectrograms are cool, but how can we use them? They contain thousands of points... how do we pick which are the most important?**\n",
    "\n",
    "As you might guess, we'll look at the spectrogram's *peaks*: points in high-energy areas. These are the most likely to survive distortions from noise, unlike ones that are close to zero and easily drowned out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2a: Peak Finding\n",
    "\n",
    "To extract these peaks, we want to find areas of the spectrogram where's there's some point that has more energy than its neighbors. To do this, we're going to need some non-linear filtering. \n",
    "\n",
    "### Max Filtering \n",
    "\n",
    "So far in this course, we have almost entirely worked with LTI systems, as they're amenable to analysis. However, LTI filtering can't help us here because a \"maximum\" operation is fundamentally non-linear, failing to satisfy the superposition property. \n",
    "\n",
    "To do our peak finding, we'll use Scipy's [`maximum_filter`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.maximum_filter.html) function with a neighborhood of 51 (the `size` parameter in the function call). \n",
    "\n",
    "For each point in our spectrogram, this filter will take our spectrogram $f(x, y)$ and output $g(x, y)$, the maximum value in a 51x51 region around the pixel. \n",
    "\n",
    "Formally,\n",
    "\n",
    "$$g(x, y) = \\max_{i,j} f(x+i, y+j) \\text{  where } -25\\le i, j \\le 25.$$\n",
    "\n",
    "### Your Job\n",
    "\n",
    "Implement the maximum filter and apply it to the spectrogram of \"Viva La Vida.\" When the neighborhood exceeds the boundary of the image, assume $f(x, y)$ is the value of the image at that point (i.e., set `mode='constant'`).\n",
    "\n",
    "\n",
    "After applying the maximum filter:\n",
    "1. Extract a boolean mask which is True when $f(x, y) = g(x, y)$, and False otherwise.\n",
    "2. To ensure these peaks are big enough, in the mask, set any peak locations with a peak below `AMP_THRESH` to zero. You can easily, but are not required to, accomplish this with a bitwise `&` operation.\n",
    "3. Use [`np.nonzero`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) to convert your mask into a set of (frequency, time) pairs. This function will return two arrays. The first is the indices along the frequency axis of the spectrogram where the peaks show up, and the second is the peak indices along the time axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEIGHBORHOOD_SIZE = 2 * 25 + 1\n",
    "AMP_THRESH = 40\n",
    "\n",
    "# TODO: Apply a Maximum Filter\n",
    "max_spect = \n",
    "\n",
    "# TODO: Compute the mask\n",
    "mask = \n",
    "\n",
    "# TODO: Filter out tiny peaks\n",
    "mask &= \n",
    "\n",
    "# TODO: Get the indices of the peaks\n",
    "freq_idx, time_idx = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the next cell and see where our peaks are! We'll label them with black dots for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6), dpi=200)\n",
    "plt.scatter(t1[time_idx], f1[freq_idx], zorder=99, color='k')\n",
    "plt.pcolormesh(t1, f1, coldplay_spect, zorder=0, cmap=\"jet\")\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.title(\"Spectrogram Peaks (for Viva La Vida)\")\n",
    "plt.xlim([0, 60])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** In Q1, we saw how most of the information in music signals is in the lower frequencies (under, say, $10 \\text{ kHz}$). How does this compare with the spectrogram peaks? Are they mostly in lower or upper half of the spectrgram? Is this what you'd expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**A:** (TODO)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2b: Hashing\n",
    "\n",
    "The peaks we've found make up what the creators of Shazam call a *constellation map*. We'll use the points in our constellation map to compute the song's fingerprints. \n",
    "\n",
    "To do this, we'll take each peak, say $(t_i, f_i)$, and chain it together with the next 15 peaks $(t_{i+1}, f_{i+1}), ..., (t_{i+15}, f_{i+15})$ by:\n",
    "1. Subtracting the times where the peaks show up, computing $t_d = t_{i+j} - t_i$.\n",
    "2. Hash the string $[f_i:f_{i+j}:t_d]$. The fingerprint is $(\\text{hash}, t_i)$.\n",
    "\n",
    "Hashing is out of the scope of this course, so we've provided the hashing function for you (`generate_hash`). It takes  arguments $f_i, f_{i+j}, t_i, t_{i+j}$ (in that order), and does steps 1 and 2, returning the fingerprint, $(\\text{hash}, t_i)$.\n",
    "\n",
    "We've provided `sorted_peaks`, a list of all the peaks sorted in increasing order by the time at which they occur in the song. Note that each element of `sorted_peaks` contains two elements, the $(f, t)$ tuple indicating where the peak occured on the spectrogram. That is, `sorted_peaks[i]` contains the tuple $(f_i, t_i)$.\n",
    "\n",
    "### Your Job\n",
    "\n",
    "Using this, your job is to, for each peak in `sorted_peaks`:\n",
    "1. Extract $(f_i, t_i)$.\n",
    "2. Iterate over the next `HASHES_PER_PEAK` peaks (if you've hit the end of the array, then stop chaining the current peak and move onto the next peak), and:\n",
    "    - Extract $(f_{i+j}, t_{i+j})$.\n",
    "    - Generate the hash using the `generate_hash` function with $f_i, f_{i+j}, t_i, t_{i+j}$.\n",
    "    - Append it to `hashes`.\n",
    "\n",
    "Remember, we only found the indices of the peaks, not the actual times and frequencies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HASHES_PER_PEAK = 15\n",
    "times, freqs = t1[time_idx], f1[freq_idx]\n",
    "sorted_peaks = sorted(zip(freqs, times), key=lambda x: x[1])\n",
    "hashes = []\n",
    "\n",
    "# TODO populate \"hashes\" with the fingerprints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first three hashes. If your code's correct, the output of running the next cell should be `[('eb046cc61c9c72cbc3e9', 1.6853333333333333), ('c3732d9a4bc79b82286e', 1.6853333333333333), ('8ff87ca5f63e8a036f46', 1.6853333333333333)]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hashes[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can easily compute the fingerprint of a signal! After fingerprinting, all we need to do is search our database for a match. If we did things correctly, the database entry we have the most fingerprints in common with should match the true song.\n",
    "\n",
    "Let's move all of this code into a single function so we can easily compute hashes for any audio signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fingerprint(audio, fs, min_distance=25, amp_thresh=40, hashes_per_peak=15):\n",
    "    NEIGHBORHOOD_SIZE = 2 * min_distance + 1\n",
    "    AMP_THRESH = amp_thresh\n",
    "    HASHES_PER_PEAK = hashes_per_peak\n",
    "    \n",
    "    audio = np.mean(audio, axis=1)\n",
    "    \n",
    "    # TODO: Compute the spectrogram of the single channel audio (Copy fr) \n",
    "\n",
    "    \n",
    "    # TODO: Apply a Maximum Filter (Copy from Q2a)\n",
    "\n",
    "    \n",
    "    # TODO: Compute the mask (Copy from Q2a)\n",
    "\n",
    "    \n",
    "    # TODO: Filter out tiny peaks (Copy from Q2a)\n",
    "\n",
    "    \n",
    "    # TODO: Get the indices of the peaks (Copy from Q2a)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    times, freqs = t1[time_idx], f1[freq_idx]\n",
    "    sorted_peaks = sorted(zip(freqs, times), key=lambda x: x[1])\n",
    "    hashes = []\n",
    "    # TODO: Compute the hashes (Copy from Q2b)\n",
    "    \n",
    "    \n",
    "    return hashes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: Testing\n",
    "As mentioned before, all we need to do now is test our system and make sure it's as robust as we think it is. Our database is stored in `database.csv`. It's columns are |Hash|t1|Song|. A production application with thousands of songs in the database would use SQL or some other querying language, but a simple CSV will suffice for our uses.\n",
    "\n",
    "Because searching through our database is more of a software problem than a Signals and Systems problem, we've provided the detection function for you. \n",
    "\n",
    "This function:\n",
    "1. Loads the CSV using pandas (a data analysis package),\n",
    "2. Fingerprints the unknown sample,\n",
    "3. Searches for matches, and\n",
    "4. Returns the song with the most matches, its confidence as a percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(audio, fs):\n",
    "    db = pd.read_csv(\"database.csv\", header=None, names=[\"Hash\", \"time\", \"Song\"])\n",
    "\n",
    "    hashes = fingerprint(audio, fs)\n",
    "    db_matches = db[db.Hash.isin(map(lambda x: x[0], hashes))]\n",
    "    if len(db_matches) == 0:\n",
    "        print(\"No Matches\")\n",
    "        return\n",
    "\n",
    "    counts = db_matches.groupby(\"Song\").size()\n",
    "    counts = counts / counts.sum()\n",
    "    return counts.idxmax(), counts.max() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3a: Basic Testing\n",
    "\n",
    "Let's see how our system does under ideal conditions (i.e, no noise). Take a 20 second segment from Viva La Vida and Mr. Brightside and call the detect function to identify it. We've already reloaded the audio for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs, coldplay = wavfile.read(\"VivaLaVida.wav\")\n",
    "fs, killers = wavfile.read(\"MrBrightside.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Detect a 20 second chunk of Coldplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Detect a 20 second chunk of Mr. Brightside\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3b: Gaussian Noise\n",
    "We want our system to be robust to different forms of noise. To start with, let's add some Gaussian noise to our audio and try to detect its origin. We'll take a 20 second chunk of Viva La Vida, add Gaussian noise with a mean and variance of 10000, and see if you can identify them. *(Hint: Checkout the [`np.random.normal`](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.normal.html) function)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add gaussian noise to some 20 sec chunk, call the result unknown_segment_gaussian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our version of Shazam should still be able to detect the song. How does it sound, though?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(unknown_segment_gaussian.T, rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It sounds terrible, and we can barely make out the music! Yet, our system still correctly identified it as *Viva La Vida*!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with *Mr. Brightside*. Now, take a 20 second chunk of the song, add Gaussian noise with a mean and variance of 10000, and see we can still identify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add gaussian noise to some 20 sec chunk, call the result unknown_segment_gaussian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the song's barely audible, but our identification system still works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(unknown_segment_gaussian.T, rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3c: Blocked Speaker\n",
    "\n",
    "What if instead of Gaussian noise, a portion of the audio just becomes zero? Arguably, this is a more realistic model of how our signal could get corrupted when dealing with music recognition. For example, somebody could move in front of the speaker, pause the music, or turn the volume down very low. \n",
    "\n",
    "Take a 20 second chunk of Viva La Vida, zero out five random 2 second chunks, and see if we can still detect the source. Use np.copy if you have copying issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO store in unknown_segment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's hear how the song sounds with these portions removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(unknown_segment.T, rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does Shazam do now? Surely it'll fail with half the clip missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect(unknown_segment, fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about *Mr. Brightside*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO store in unknown_segment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(unknown_segment.T, rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect(unknown_segment, fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our system is pretty robust!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Comments\n",
    "There are many ways to improve our Shazam system. Many of them have to do with how we compute our spectrogram as well as the various parameters we introduced such as `NEIGHBORHOOD_SIZE`, `AMP_THRESH`, and `HASHES_PER_PEAK`. But, for the most part, this is how Shazam works!\n",
    "\n",
    "The original Shazam paper uses a different method for matching the fingerprints of audio instead of a simple \"most matches => song\" scheme, but for our limited database, this works just fine. Check out the original paper if you are curious. If you'd like, you can use the following cells to load your own songs into the database (as long as they are wav files) and try to identify samples of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def add_to_db(filename):\n",
    "    fs, audio = wavfile.read(filename)\n",
    "    hashes = fingerprint(audio, fs)\n",
    "    with open('database.csv', mode='a') as db_file:\n",
    "        db_writer = csv.writer(db_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "        for hash_pair in hashes:\n",
    "            db_writer.writerow([hash_pair[0], hash_pair[1], filename])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to add a song to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_wav_filepath = # path to any WAV file you want to add to the database\n",
    "add_to_db(my_wav_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] *An industrial strength audio search algorithm.* [[Link](http://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf)].  \n",
    "[2] *Audio fingerprinting with Python and Numpy.* [[Link](https://willdrevo.com/fingerprinting-and-audio-recognition-with-python/)]."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
